{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3236c6-91fe-4996-9c10-7fe2f48e36f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Input:\n",
      "Buy when the close price is above the 20-day moving average and volume is above 1M. Exit when RSI(14) is below 30.\n",
      "\n",
      "Generated DSL:\n",
      "ENTRY:\n",
      "close > sma(close,0) AND volume > 1\n",
      "EXIT:\n",
      "rsi(close,14) < 30\n",
      "\n",
      "Parsed Blocks:\n",
      "ENTRY: close > sma(close,0) AND volume > 1\n",
      "EXIT:  rsi(close,14) < 30\n",
      "\n",
      "AST (entry): BinOp(op='GT', left=Identifier(name='close'), right=FuncCall(name='sma', args=[Identifier(name='close'), Number(value=0.0)]))\n",
      "AST (exit): BinOp(op='LT', left=FuncCall(name='rsi', args=[Identifier(name='close'), Number(value=14.0)]), right=Number(value=30.0))\n",
      "\n",
      "Signals head:\n",
      "            entry   exit\n",
      "date                    \n",
      "2023-01-01  False  False\n",
      "2023-01-02  False  False\n",
      "2023-01-03  False  False\n",
      "2023-01-04  False  False\n",
      "2023-01-05  False  False\n",
      "2023-01-06  False  False\n",
      "2023-01-07  False  False\n",
      "2023-01-08  False  False\n",
      "2023-01-09  False  False\n",
      "2023-01-10  False  False\n",
      "\n",
      "=== Backtest Report ===\n",
      "Trades: 0\n",
      "Total PnL: 0.0000\n",
      "Total Return: 0.00%\n",
      "Max Drawdown: 0.00%\n",
      "Trade Log:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "strategy_pipeline.py\n",
    "\n",
    "Single-file NL -> DSL -> AST -> Python -> Backtest pipeline.\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas numpy\n",
    "\n",
    "Usage:\n",
    "    python strategy_pipeline.py\n",
    "\n",
    "It will run a sample demonstration at the bottom. Adjust `NL_INPUT` or\n",
    "call functions programmatically to test other rules.\n",
    "\n",
    " Sakthiyogesh\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Union, Tuple, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# Part A: Simple NL -> DSL\n",
    "# -----------------------\n",
    "def nl_to_dsl(nl: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a small set of natural-language phrases into the DSL form.\n",
    "    This is intentionally conservative: supports patterns like the examples in the PDF.\n",
    "    \"\"\"\n",
    "    s = nl.strip().lower()\n",
    "    # common phrase -> tokens\n",
    "    # 1. buy/enter vs exit\n",
    "    # We'll produce two-block DSL: ENTRY: ... \\n EXIT: ...\n",
    "    entry_clauses = []\n",
    "    exit_clauses = []\n",
    "\n",
    "    # patterns for entry\n",
    "    # \"buy when the close price is above the 20-day moving average and volume is above 1 million.\"\n",
    "    m = re.search(r'(buy|enter).+close.+above.+(\\d+)[- ]?day', s)\n",
    "    if m:\n",
    "        days = int(m.group(2))\n",
    "        entry_clauses.append(f\"close > sma(close,{days})\")\n",
    "\n",
    "    m2 = re.search(r'volume.+above\\s+([\\d,.kKmM]+)', s)\n",
    "    if m2:\n",
    "        val = parse_human_number(m2.group(1))\n",
    "        entry_clauses.append(f\"volume > {int(val)}\")\n",
    "\n",
    "    # cross above yesterday's high\n",
    "    if \"cross\" in s and \"yesterday\" in s:\n",
    "        entry_clauses.append(\"crosses_above(close, yesterday_high)\")\n",
    "\n",
    "    # RSI exit pattern\n",
    "    m3 = re.search(r'rsi\\s*\\(?\\s*(\\d+)\\s*\\)?.*below\\s*(\\d+)', s)\n",
    "    if m3:\n",
    "        rsi_p = int(m3.group(1))\n",
    "        val = int(m3.group(2))\n",
    "        exit_clauses.append(f\"rsi(close,{rsi_p}) < {val}\")\n",
    "\n",
    "    # fallback: try to detect simple comparisons\n",
    "    if not entry_clauses and not exit_clauses:\n",
    "        # try to extract \"close > sma(close,20) and volume > 1000000\"\n",
    "        # naive convert numbers like 1M to 1000000\n",
    "        s2 = s\n",
    "        s2 = s2.replace(\"greater than\", \">\").replace(\"less than\", \"<\")\n",
    "        s2 = s2.replace(\"and\", \"AND\").replace(\"or\", \"OR\")\n",
    "        # digits rewrite\n",
    "        s2 = re.sub(r'([\\d,.]+)\\s*m\\b', lambda mo: str(parse_human_number(mo.group(1) + 'm')), s2)\n",
    "        # look for rsi pattern\n",
    "        if \"rsi\" in s2 and \"exit\" in s2:\n",
    "            exit_clauses.append(s2)\n",
    "        elif \"buy\" in s2 or \"entry\" in s2:\n",
    "            entry_clauses.append(s2)\n",
    "\n",
    "    # build DSL\n",
    "    dsl_lines = []\n",
    "    if entry_clauses:\n",
    "        dsl_lines.append(\"ENTRY:\")\n",
    "        dsl_lines.append(\" AND \".join(entry_clauses))\n",
    "    if exit_clauses:\n",
    "        dsl_lines.append(\"EXIT:\")\n",
    "        dsl_lines.append(\" AND \".join(exit_clauses))\n",
    "    if not dsl_lines:\n",
    "        raise ValueError(\"Could not map NL to DSL with current simple rules. Provide a clearer rule.\")\n",
    "    return \"\\n\".join(dsl_lines)\n",
    "\n",
    "def parse_human_number(s: str) -> float:\n",
    "    s = s.strip().lower().replace(',', '')\n",
    "    if s.endswith('m'):\n",
    "        return float(s[:-1]) * 1_000_000\n",
    "    if s.endswith('k'):\n",
    "        return float(s[:-1]) * 1_000\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return float(re.sub(r'[^\\d.]', '', s))\n",
    "\n",
    "# -----------------------\n",
    "# Part B: DSL Tokenizer + Parser -> AST\n",
    "# -----------------------\n",
    "\n",
    "# We'll support:\n",
    "# - IDENT (close, open, high, low, volume)\n",
    "# - NUM (integer)\n",
    "# - FUNCTION calls like sma(close,20), rsi(close,14)\n",
    "# - operators: > < >= <= ==, AND, OR\n",
    "# - special function: crosses_above(a,b)\n",
    "# - parentheses\n",
    "\n",
    "Token = Tuple[str, str]\n",
    "\n",
    "token_specification = [\n",
    "    ('NUMBER',   r'\\d+(\\.\\d+)?'),   # Integer or decimal number\n",
    "    ('FUNC',     r'[A-Za-z_][A-Za-z0-9_]*\\s*\\('),  # function name + '('\n",
    "    ('ID',       r'[A-Za-z_][A-Za-z0-9_]*'), # identifiers\n",
    "    ('GE',       r'>='),\n",
    "    ('LE',       r'<='),\n",
    "    ('GT',       r'>'),\n",
    "    ('LT',       r'<'),\n",
    "    ('EQ',       r'=='),\n",
    "    ('LPAREN',   r'\\('),\n",
    "    ('RPAREN',   r'\\)'),\n",
    "    ('COMMA',    r','),\n",
    "    ('AND',      r'\\bAND\\b'),\n",
    "    ('OR',       r'\\bOR\\b'),\n",
    "    ('WS',       r'\\s+'),\n",
    "    ('OTHER',    r'.'),\n",
    "]\n",
    "\n",
    "tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)\n",
    "master_re = re.compile(tok_regex, flags=re.IGNORECASE)\n",
    "\n",
    "def tokenize(s: str) -> List[Token]:\n",
    "    pos = 0\n",
    "    tokens = []\n",
    "    while pos < len(s):\n",
    "        m = master_re.match(s, pos)\n",
    "        if not m:\n",
    "            raise SyntaxError(f\"Unexpected text at pos {pos}: {s[pos:]}\")\n",
    "        typ = m.lastgroup\n",
    "        val = m.group(typ)\n",
    "        pos = m.end()\n",
    "        if typ == 'WS':\n",
    "            continue\n",
    "        if typ == 'FUNC':\n",
    "            # FUNC includes trailing '('; normalize\n",
    "            fname = val[:-1].strip()\n",
    "            tokens.append(('FUNC', fname))\n",
    "            tokens.append(('LPAREN','('))\n",
    "            continue\n",
    "        tokens.append((typ, val))\n",
    "    return tokens\n",
    "\n",
    "# AST node classes\n",
    "@dataclass\n",
    "class ASTNode:\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class BinOp(ASTNode):\n",
    "    op: str\n",
    "    left: ASTNode\n",
    "    right: ASTNode\n",
    "\n",
    "@dataclass\n",
    "class FuncCall(ASTNode):\n",
    "    name: str\n",
    "    args: List[ASTNode]\n",
    "\n",
    "@dataclass\n",
    "class Identifier(ASTNode):\n",
    "    name: str\n",
    "\n",
    "@dataclass\n",
    "class Number(ASTNode):\n",
    "    value: float\n",
    "\n",
    "# Parser: recursive descent for expressions with precedence:\n",
    "# comparisons (>,<,>=,<=,==) lowest than AND/OR? Actually boolean ops lower precedence than comparisons.\n",
    "# We'll parse comparisons as nodes that return booleans; AND/OR combine them.\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, tokens: List[Token]):\n",
    "        self.tokens = tokens\n",
    "        self.pos = 0\n",
    "\n",
    "    def peek(self) -> Token:\n",
    "        return self.tokens[self.pos] if self.pos < len(self.tokens) else ('EOF','')\n",
    "\n",
    "    def eat(self, expected_type: str = None) -> Token:\n",
    "        tok = self.peek()\n",
    "        if expected_type and tok[0].upper() != expected_type.upper():\n",
    "            raise SyntaxError(f\"Expected {expected_type} but got {tok}\")\n",
    "        self.pos += 1\n",
    "        return tok\n",
    "\n",
    "    def parse(self) -> ASTNode:\n",
    "        return self.parse_or()\n",
    "\n",
    "    def parse_or(self):\n",
    "        node = self.parse_and()\n",
    "        while True:\n",
    "            tok = self.peek()\n",
    "            if tok[0].upper() == 'OR':\n",
    "                self.eat('OR')\n",
    "                right = self.parse_and()\n",
    "                node = BinOp('OR', node, right)\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def parse_and(self):\n",
    "        node = self.parse_comparison()\n",
    "        while True:\n",
    "            tok = self.peek()\n",
    "            if tok[0].upper() == 'AND':\n",
    "                self.eat('AND')\n",
    "                right = self.parse_comparison()\n",
    "                node = BinOp('AND', node, right)\n",
    "            else:\n",
    "                break\n",
    "        return node\n",
    "\n",
    "    def parse_comparison(self):\n",
    "        left = self.parse_term()\n",
    "        tok = self.peek()\n",
    "        if tok[0] in ('GT','LT','GE','LE','EQ'):\n",
    "            op = self.eat()[0]\n",
    "            right = self.parse_term()\n",
    "            return BinOp(op, left, right)\n",
    "        return left\n",
    "\n",
    "    def parse_term(self):\n",
    "        tok = self.peek()\n",
    "        if tok[0] == 'LPAREN':\n",
    "            self.eat('LPAREN')\n",
    "            node = self.parse()\n",
    "            self.eat('RPAREN')\n",
    "            return node\n",
    "        if tok[0] == 'FUNC':\n",
    "            name = self.eat('FUNC')[1]\n",
    "            args = self.parse_arglist()\n",
    "            return FuncCall(name.lower(), args)\n",
    "        if tok[0] == 'ID':\n",
    "            val = self.eat('ID')[1]\n",
    "            return Identifier(val.lower())\n",
    "        if tok[0] == 'NUMBER':\n",
    "            v = float(self.eat('NUMBER')[1])\n",
    "            return Number(v)\n",
    "        # special support for \"yesterday_high\" token or similar IDs\n",
    "        raise SyntaxError(f\"Unexpected token {tok} at position {self.pos}\")\n",
    "\n",
    "    def parse_arglist(self) -> List[ASTNode]:\n",
    "        args = []\n",
    "        # we are after LPAREN\n",
    "        # eat '(' already consumed by tokenizer as separate token (we did)\n",
    "        # parse until RPAREN\n",
    "        # NOTE: parser created FUNC token then left LPAREN token; here we assume current token is after that LPAREN\n",
    "        # But our tokenizer emitted LPAREN already; ensure to consume if present\n",
    "        # We'll expect '(' already consumed by parse_term by previous code; but for simplicity:\n",
    "        # if current token is RPAREN, return []\n",
    "        # Actually parse_term called FUNC then parse_arglist; after eating FUNC we left LPAREN in stream and\n",
    "        # didn't consume it; so we need to eat LPAREN now:\n",
    "        if self.peek()[0] == 'LPAREN':\n",
    "            self.eat('LPAREN')\n",
    "        while True:\n",
    "            if self.peek()[0] == 'RPAREN':\n",
    "                self.eat('RPAREN')\n",
    "                break\n",
    "            arg = self.parse()\n",
    "            args.append(arg)\n",
    "            if self.peek()[0] == 'COMMA':\n",
    "                self.eat('COMMA')\n",
    "                continue\n",
    "            elif self.peek()[0] == 'RPAREN':\n",
    "                self.eat('RPAREN')\n",
    "                break\n",
    "            else:\n",
    "                # If expression doesn't have comma but next is RPAREN eventually, loop will break\n",
    "                pass\n",
    "        return args\n",
    "\n",
    "def parse_dsl_block(text: str) -> Dict[str,str]:\n",
    "    \"\"\"\n",
    "    Splits DSL into ENTRY/EXIT blocks and returns dict {'entry': '...', 'exit': '...'}\n",
    "    \"\"\"\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "    cur = None\n",
    "    blocks = {}\n",
    "    for ln in lines:\n",
    "        up = ln.upper()\n",
    "        if up.startswith(\"ENTRY:\"):\n",
    "            cur = 'entry'\n",
    "            rest = ln[len(\"ENTRY:\"):].strip()\n",
    "            blocks[cur] = rest if rest else \"\"\n",
    "            continue\n",
    "        if up.startswith(\"EXIT:\"):\n",
    "            cur = 'exit'\n",
    "            rest = ln[len(\"EXIT:\"):].strip()\n",
    "            blocks[cur] = rest if rest else \"\"\n",
    "            continue\n",
    "        if cur is None:\n",
    "            continue\n",
    "        # append\n",
    "        blocks[cur] = blocks.get(cur, \"\") + \" \" + ln\n",
    "    return blocks\n",
    "\n",
    "def build_ast_from_text(expr_text: str) -> ASTNode:\n",
    "    tokens = tokenize(expr_text)\n",
    "    p = Parser(tokens)\n",
    "    ast = p.parse()\n",
    "    return ast\n",
    "\n",
    "# -----------------------\n",
    "# Part C: AST -> pandas expression (code generator)\n",
    "# -----------------------\n",
    "\n",
    "# We'll generate a Python function that, given df, returns signals DataFrame with boolean columns 'entry' and 'exit'.\n",
    "# To keep it readable, we will create helper functions and then evaluate expressions.\n",
    "\n",
    "def ast_to_python_expr(ast: ASTNode, temp_vars: Dict[str,int]=None) -> str:\n",
    "    \"\"\"\n",
    "    Convert AST into a python expression string that returns a pandas Series (boolean or numeric).\n",
    "    Uses df[...] for series and helper functions (sma, rsi, cross_above).\n",
    "    \"\"\"\n",
    "    if temp_vars is None:\n",
    "        temp_vars = {'v':0}\n",
    "    if isinstance(ast, Number):\n",
    "        return repr(ast.value)\n",
    "    if isinstance(ast, Identifier):\n",
    "        name = ast.name.lower()\n",
    "        # map some aliases\n",
    "        if name == 'yesterday_high':\n",
    "            return \"df['high'].shift(1)\"\n",
    "        if name in ('open','high','low','close','volume'):\n",
    "            return f\"df['{name}']\"\n",
    "        # allow 'close[-1]' style? not here\n",
    "        return f\"df['{name}']\"\n",
    "    if isinstance(ast, FuncCall):\n",
    "        n = ast.name.lower()\n",
    "        args_expr = [ast_to_python_expr(a, temp_vars) for a in ast.args]\n",
    "        if n == 'sma':\n",
    "            # sma(series, period)\n",
    "            return f\"sma({args_expr[0]}, int({args_expr[1]}))\"\n",
    "        if n == 'rsi':\n",
    "            return f\"rsi({args_expr[0]}, int({args_expr[1]}))\"\n",
    "        if n in ('crosses_above','cross_above','cross_above'):\n",
    "            return f\"cross_above({args_expr[0]},{args_expr[1]})\"\n",
    "        # generic function fallback (not expected)\n",
    "        return f\"{n}({', '.join(args_expr)})\"\n",
    "    if isinstance(ast, BinOp):\n",
    "        op = ast.op\n",
    "        left = ast_to_python_expr(ast.left, temp_vars)\n",
    "        right = ast_to_python_expr(ast.right, temp_vars)\n",
    "        # map op tokens to python/pandas\n",
    "        if op == 'AND':\n",
    "            return f\"({left}) & ({right})\"\n",
    "        if op == 'OR':\n",
    "            return f\"({left}) | ({right})\"\n",
    "        if op == 'GT' or op == '>':\n",
    "            return f\"({left}) > ({right})\"\n",
    "        if op == 'LT' or op == '<':\n",
    "            return f\"({left}) < ({right})\"\n",
    "        if op == 'GE' or op == '>=':\n",
    "            return f\"({left}) >= ({right})\"\n",
    "        if op == 'LE' or op == '<=':\n",
    "            return f\"({left}) <= ({right})\"\n",
    "        if op == 'EQ' or op == '==':\n",
    "            return f\"({left}) == ({right})\"\n",
    "        # catch-case: sometimes tokens are actual symbols\n",
    "        if op in ('>','<','>=','<=','=='):\n",
    "            return f\"({left}) {op} ({right})\"\n",
    "        # fallback\n",
    "        return f\"({left}) {op} ({right})\"\n",
    "\n",
    "def generate_signal_function(entry_ast: Union[ASTNode,None], exit_ast: Union[ASTNode,None]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a string with a Python function definition 'generate_signals(df)' which uses helper functions.\n",
    "    \"\"\"\n",
    "    body_lines = []\n",
    "    body_lines.append(\"def generate_signals(df):\")\n",
    "    body_lines.append(\"    # expects df with columns: open,high,low,close,volume and datetime index or 'date' column\")\n",
    "    body_lines.append(\"    signals = pd.DataFrame(index=df.index)\")\n",
    "    if entry_ast:\n",
    "        expr = ast_to_python_expr(entry_ast)\n",
    "        body_lines.append(f\"    signals['entry'] = {expr}\")\n",
    "    else:\n",
    "        body_lines.append(\"    signals['entry'] = False\")\n",
    "    if exit_ast:\n",
    "        expr2 = ast_to_python_expr(exit_ast)\n",
    "        body_lines.append(f\"    signals['exit'] = {expr2}\")\n",
    "    else:\n",
    "        body_lines.append(\"    signals['exit'] = False\")\n",
    "    body_lines.append(\"    # ensure booleans\")\n",
    "    body_lines.append(\"    signals['entry'] = signals['entry'].fillna(False).astype(bool)\")\n",
    "    body_lines.append(\"    signals['exit'] = signals['exit'].fillna(False).astype(bool)\")\n",
    "    body_lines.append(\"    return signals\")\n",
    "    return \"\\n\".join(\"    \" + ln if i>0 else ln for i,ln in enumerate(body_lines))\n",
    "\n",
    "# -----------------------\n",
    "# Part D: Indicator helpers and cross detection\n",
    "# -----------------------\n",
    "\n",
    "def sma(series: pd.Series, period: int) -> pd.Series:\n",
    "    period = max(1, int(period))   # Prevent 0 or negative values\n",
    "    return series.rolling(period, min_periods=1).mean()\n",
    "\n",
    "def rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
    "    # classic RSI (Wilder)\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0.0)\n",
    "    down = -1 * delta.clip(upper=0.0)\n",
    "    # Wilder smoothing\n",
    "    ma_up = up.ewm(alpha=1/period, adjust=False, min_periods=period).mean()\n",
    "    ma_down = down.ewm(alpha=1/period, adjust=False, min_periods=period).mean()\n",
    "    rs = ma_up / (ma_down + 1e-12)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def cross_above(s1: pd.Series, s2: pd.Series) -> pd.Series:\n",
    "    prev = s1.shift(1)\n",
    "    prev2 = s2.shift(1)\n",
    "    return (s1 > s2) & (prev <= prev2)\n",
    "\n",
    "# -----------------------\n",
    "# Part E: Backtest simulator\n",
    "# -----------------------\n",
    "\n",
    "@dataclass\n",
    "class Trade:\n",
    "    entry_idx: Any\n",
    "    exit_idx: Any\n",
    "    entry_price: float\n",
    "    exit_price: float\n",
    "    pnl: float\n",
    "    ret: float\n",
    "\n",
    "def run_backtest(df: pd.DataFrame, signals: pd.DataFrame, verbose: bool=False) -> Dict:\n",
    "    \"\"\"\n",
    "    Very simple backtester:\n",
    "    - Start flat\n",
    "    - Enter at next bar's open when signals['entry'] becomes True (for simplicity we enter at close of same bar)\n",
    "    - Exit at next bar's close when signals['exit'] True\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    trades: List[Trade] = []\n",
    "    entry_price = None\n",
    "    entry_idx = None\n",
    "\n",
    "    for idx in df.index:\n",
    "        row_entry = signals.loc[idx, 'entry']\n",
    "        row_exit  = signals.loc[idx, 'exit']\n",
    "        price = float(df.loc[idx, 'close'])\n",
    "\n",
    "        if pos == 0 and row_entry:\n",
    "            # enter long at current close (simple)\n",
    "            pos = 1\n",
    "            entry_price = price\n",
    "            entry_idx = idx\n",
    "            if verbose:\n",
    "                print(f\"ENTER at {idx} price {price}\")\n",
    "            continue  # don't exit same bar\n",
    "\n",
    "        if pos == 1 and row_exit:\n",
    "            exit_price = price\n",
    "            exit_idx = idx\n",
    "            pnl = exit_price - entry_price\n",
    "            ret = pnl / entry_price if entry_price != 0 else 0.0\n",
    "            trades.append(Trade(entry_idx, exit_idx, entry_price, exit_price, pnl, ret))\n",
    "            pos = 0\n",
    "            entry_price = None\n",
    "            entry_idx = None\n",
    "            if verbose:\n",
    "                print(f\"EXIT at {idx} price {exit_price} pnl {pnl}\")\n",
    "            continue\n",
    "\n",
    "    # if still in position at the end, close at final close\n",
    "    if pos == 1:\n",
    "        final_price = float(df.iloc[-1]['close'])\n",
    "        exit_idx = df.index[-1]\n",
    "        pnl = final_price - entry_price\n",
    "        ret = pnl / entry_price\n",
    "        trades.append(Trade(entry_idx, exit_idx, entry_price, final_price, pnl, ret))\n",
    "\n",
    "    # metrics\n",
    "    total_pnl = sum(t.pnl for t in trades)\n",
    "    total_return = (np.prod([1 + t.ret for t in trades]) - 1) if trades else 0.0\n",
    "    equity = [0.0]\n",
    "    # simple equity curve build: start at 1.0, apply returns sequentially\n",
    "    eq = 1.0\n",
    "    eq_curve = [eq]\n",
    "    for t in trades:\n",
    "        eq = eq * (1 + t.ret)\n",
    "        eq_curve.append(eq)\n",
    "    # max drawdown (from equity curve)\n",
    "    eq_arr = np.array(eq_curve)\n",
    "    peak = np.maximum.accumulate(eq_arr)\n",
    "    dd = (eq_arr - peak) / peak\n",
    "    max_dd = float(dd.min()) if len(dd)>0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        'trades': trades,\n",
    "        'total_pnl': total_pnl,\n",
    "        'total_return': total_return,\n",
    "        'num_trades': len(trades),\n",
    "        'max_drawdown': max_dd,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Pretty print trades\n",
    "def print_backtest_report(results):\n",
    "    print(\"=== Backtest Report ===\")\n",
    "    print(f\"Trades: {results['num_trades']}\")\n",
    "    print(f\"Total PnL: {results['total_pnl']:.4f}\")\n",
    "    print(f\"Total Return: {results['total_return']*100:.2f}%\")\n",
    "    print(f\"Max Drawdown: {results['max_drawdown']*100:.2f}%\")\n",
    "    print(\"Trade Log:\")\n",
    "    for t in results['trades']:\n",
    "        print(f\"- Enter: {t.entry_idx} @{t.entry_price}  Exit: {t.exit_idx} @{t.exit_price}  PnL: {t.pnl:.4f}  Return: {t.ret*100:.2f}%\")\n",
    "\n",
    "# -----------------------\n",
    "# Part F: End-to-End demo\n",
    "# -----------------------\n",
    "\n",
    "SAMPLE_DATA = \"\"\"date,open,high,low,close,volume\n",
    "2023-01-01,100,105,99,103,900000\n",
    "2023-01-02,103,108,101,107,1200000\n",
    "2023-01-03,107,110,106,109,1300000\n",
    "2023-01-04,109,112,108,111,1500000\n",
    "2023-01-05,111,115,110,114,2000000\n",
    "2023-01-06,114,117,113,116,1800000\n",
    "2023-01-07,116,119,115,118,1600000\n",
    "2023-01-08,118,121,117,120,1700000\n",
    "2023-01-09,120,122,119,121,900000\n",
    "2023-01-10,121,125,120,124,2000000\n",
    "\"\"\"\n",
    "\n",
    "def load_sample_df(csv_text: str) -> pd.DataFrame:\n",
    "    from io import StringIO\n",
    "    df = pd.read_csv(StringIO(csv_text), parse_dates=['date'])\n",
    "    df = df.set_index('date')\n",
    "    return df\n",
    "\n",
    "def end_to_end(nl_input: str, df: pd.DataFrame):\n",
    "    print(\"Natural Language Input:\")\n",
    "    print(nl_input)\n",
    "    dsl = nl_to_dsl(nl_input)\n",
    "    print(\"\\nGenerated DSL:\")\n",
    "    print(dsl)\n",
    "    blocks = parse_dsl_block(dsl)\n",
    "    entry_text = blocks.get('entry','').strip()\n",
    "    exit_text = blocks.get('exit','').strip()\n",
    "    print(\"\\nParsed Blocks:\")\n",
    "    print(\"ENTRY:\", entry_text)\n",
    "    print(\"EXIT: \", exit_text)\n",
    "\n",
    "    entry_ast = build_ast_from_text(entry_text) if entry_text else None\n",
    "    exit_ast  = build_ast_from_text(exit_text) if exit_text else None\n",
    "\n",
    "    # show ASTs (simple)\n",
    "    print(\"\\nAST (entry):\", entry_ast)\n",
    "    print(\"AST (exit):\", exit_ast)\n",
    "\n",
    "    # generate python function text\n",
    "    func_code = generate_signal_function(entry_ast, exit_ast)\n",
    "    # build full runtime environment\n",
    "    runtime_globals = {\n",
    "        'pd': pd,\n",
    "        'np': np,\n",
    "        'sma': sma,\n",
    "        'rsi': rsi,\n",
    "        'cross_above': cross_above,\n",
    "        'df': df\n",
    "    }\n",
    "    # exec function\n",
    "    exec(func_code, runtime_globals)\n",
    "    generate_signals = runtime_globals['generate_signals']\n",
    "    signals = generate_signals(df)\n",
    "\n",
    "    print(\"\\nSignals head:\")\n",
    "    print(signals.head(10))\n",
    "\n",
    "    results = run_backtest(df, signals)\n",
    "    print()\n",
    "    print_backtest_report(results)\n",
    "    return {\n",
    "        'dsl': dsl,\n",
    "        'entry_ast': entry_ast,\n",
    "        'exit_ast': exit_ast,\n",
    "        'signals': signals,\n",
    "        'results': results,\n",
    "        'func_code': func_code\n",
    "    }\n",
    "\n",
    "# If run as script, demo:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example natural language input\n",
    "    NL_INPUT = \"Buy when the close price is above the 20-day moving average and volume is above 1M. Exit when RSI(14) is below 30.\"\n",
    "    df = load_sample_df(SAMPLE_DATA)\n",
    "    out = end_to_end(NL_INPUT, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec87d8-abdb-4517-b9c0-111cb1cec24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0758fe-8327-40d2-8325-ba09a5e68600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
